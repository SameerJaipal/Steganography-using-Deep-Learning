{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install unicodedata\n",
        "!pip install transliterate\n",
        "!pip install transformers\n",
        "!pip install transformers[torch]\n",
        "!pip install accelerate -U\n",
        "!pip install datasets\n",
        "!pip install evaluate\n",
        "from datasets import load_dataset\n",
        "from transformers import pipeline, AutoTokenizer, DataCollatorForLanguageModeling, AutoModelForCausalLM, TrainingArguments, Trainer, TextDataset\n",
        "import math\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset"
      ],
      "metadata": {
        "id": "UF-PipIK6kfl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7dee3ba8-aadc-4558-aa69-791864fb24a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement unicodedata (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for unicodedata\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: transliterate in /usr/local/lib/python3.10/dist-packages (1.10.2)\n",
            "Requirement already satisfied: six>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from transliterate) (1.16.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n",
            "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.1)\n",
            "Requirement already satisfied: torch!=1.12.0,>=1.10 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.1.0+cu118)\n",
            "Requirement already satisfied: accelerate>=0.20.3 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.20.3->transformers[torch]) (5.9.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers[torch]) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers[torch]) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (2.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch!=1.12.0,>=1.10->transformers[torch]) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=1.12.0,>=1.10->transformers[torch]) (1.3.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.25.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu118)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.19.4)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.18.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.19.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.3)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.18.0->datasets) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.18.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.11.17)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (0.4.1)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.15.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.23.5)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2023.6.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.19.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (23.2)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.18.0)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (9.0.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (0.6)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.9.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2023.11.17)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2023.3.post1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.3)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "sqRho-HyOs8E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60c678b5-ecb4-435c-e436-427a9d2623f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading the text generation model"
      ],
      "metadata": {
        "id": "EpafzzoeeDER"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\", from_tf=True)"
      ],
      "metadata": {
        "id": "hsIrUDlCQhmY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "216231fa-e75f-4b51-f782-7f398b48bdad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All TF 2.0 model weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "Some weights of GPT2LMHeadModel were not initialized from the TF 2.0 model and are newly initialized: ['lm_head.weight', 'lm_head.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_save_name = 'urdu-textgen5.pt'\n",
        "path = F\"/content/drive/My Drive/deep/urdu-textgen5.pt\"\n",
        "model.load_state_dict(torch.load(path))\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
        "if tokenizer.pad_token is None:\n",
        "  tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "BojJp-ieDyIk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function to generate text given a word"
      ],
      "metadata": {
        "id": "FB7c3b_PeOeS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenizer = AutoTokenizer.from_pretrained(\"hadidev/gpt2-urdu-smallest\")\n",
        "# model = AutoModelForCausalLM.from_pretrained(\"hadidev/gpt2-urdu-smallest\", from_tf=True)"
      ],
      "metadata": {
        "id": "aENAE3qHpuyY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gen_word(prompt):\n",
        "  inputs = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "  outputs = model.generate(inputs, max_new_tokens=10, do_sample=True, top_k=50, top_p=0.95)\n",
        "  return tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "\n",
        "gen_word(\"اس کے بارے میں بات\")"
      ],
      "metadata": {
        "id": "CH-ittn_N_-c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39f5ff7b-c950-4880-df28-723d0325d8f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['اس کے بارے میں بات ی   ب ی']"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Converting the generated word into Baudot code"
      ],
      "metadata": {
        "id": "7zf47D3TeXPE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "class Stegonography:\n",
        "    def __init__(self):\n",
        "        self.keywords = list()\n",
        "        self.ungrouped_keys = list()\n",
        "        self.groups = dict()\n",
        "        self.baudot_code = dict()\n",
        "        self.urdu_to_baudot_mapping = {\n",
        "                                        'ا': '000000',\n",
        "                                        'ب': '000001',\n",
        "                                        'پ': '000010',\n",
        "                                        'ت': '000011',\n",
        "                                        'ٹ': '000100',\n",
        "                                        'ث': '000101',\n",
        "                                        'ج': '000110',\n",
        "                                        'چ': '000111',\n",
        "                                        'ح': '001000',\n",
        "                                        'خ': '001001',\n",
        "                                        'د': '001010',\n",
        "                                        'ڈ': '001011',\n",
        "                                        'ذ': '001100',\n",
        "                                        'ر': '001101',\n",
        "                                        'ڑ': '001110',\n",
        "                                        'ز': '001111',\n",
        "                                        'ژ': '010000',\n",
        "                                        'س': '010001',\n",
        "                                        'ش': '010010',\n",
        "                                        'ص': '010011',\n",
        "                                        'ض': '010100',\n",
        "                                        'ط': '010101',\n",
        "                                        'ظ': '010110',\n",
        "                                        'ع': '010111',\n",
        "                                        'غ': '011000',\n",
        "                                        'ف': '011001',\n",
        "                                        'ق': '011010',\n",
        "                                        'ک': '011011',\n",
        "                                        'گ': '011100',\n",
        "                                        'ل': '011101',\n",
        "                                        'م': '011110',\n",
        "                                        'ن': '011111',\n",
        "                                        'ں': '100000',\n",
        "                                        'ھ': '100001',\n",
        "                                        'و': '100010',\n",
        "                                        'ء': '100011',\n",
        "                                        'ی': '100100',\n",
        "                                        'ے': '100101',\n",
        "                                        'ئ': '100110',\n",
        "                                        'ہ': '100111',\n",
        "                                        'ۓ': '101001',\n",
        "                                        'آ': '101010',\n",
        "                                        'ؤ': '101011'\n",
        "                                    }\n",
        "\n",
        "\n",
        "\n",
        "    def set_keyword(self, keyword):\n",
        "        self.keywords.append(keyword)\n",
        "        self.ungrouped_keys.append(keyword)\n",
        "    def get_group(self):\n",
        "        for key in self.ungrouped_keys:\n",
        "            # index = np.arange(len(key))\n",
        "            # letters = np.array([*key])\n",
        "            # letters = np.unique(letters)\n",
        "            # np.random.shuffle(letters)\n",
        "            # group_0 = letters[:len(letters)//2].tolist()\n",
        "            # group_1 = letters[len(letters)//2:].tolist()\n",
        "            # self.groups.update({key:[group_0,group_1 ]})\n",
        "            # self.ungrouped_keys.remove(key)\n",
        "\n",
        "\n",
        "            letters = np.array([*'کیاآپکومیںکچھ'])\n",
        "            letters = np.unique(letters)\n",
        "            np.random.shuffle(letters)\n",
        "            group_0 = letters[:len(letters)//2].tolist()\n",
        "            group_1 = letters[len(letters)//2:].tolist()\n",
        "            self.groups.update({key:[group_0,group_1 ]})\n",
        "            self.ungrouped_keys.remove(key)\n",
        "\n",
        "    def encode(self):\n",
        "        for keyword in self.keywords:\n",
        "            print(keyword)\n",
        "            code = ''\n",
        "            letters = [*keyword]\n",
        "            for letter in letters:\n",
        "\n",
        "                if letter in list(self.urdu_to_baudot_mapping.keys()):\n",
        "                    print(f'The code is \\n{code} and The letter = {letter}')\n",
        "\n",
        "                    code += self.urdu_to_baudot_mapping[letter]\n",
        "                else:\n",
        "                    print(f'Please use a letter recognized by the dictionary, {letter}')\n",
        "            code = code\n",
        "            self.baudot_code[keyword] = (code)\n",
        "        return\n",
        "\n",
        "    def decode(self, baudot ):\n",
        "        values = list(self.urdu_to_baudot_mapping.values())\n",
        "        keys = list(self.urdu_to_baudot_mapping.keys())\n",
        "        decoded = []\n",
        "        for i in range(0, int(len(baudot)),6):\n",
        "            code = (baudot[i:i+6])\n",
        "\n",
        "\n",
        "            if code in values:\n",
        "                j = values.index(code)\n",
        "                decoded.append(keys[j])\n",
        "                print(f'the code is {code} and the letter is {keys[j]}')\n",
        "        print(decoded)\n",
        "\n",
        "    def split_urdu_word(self, word):\n",
        "      letters = [letter for letter in word]\n",
        "      return letters\n",
        "\n",
        "    def check_word(self, testword, baudot_code, grp_1, grp_0):\n",
        "        letters = self.split_urdu_word(testword)\n",
        "        print(letters, testword)\n",
        "        # print(tst_word)\n",
        "        # print(letters)\n",
        "        lgth = 0\n",
        "        error = 0\n",
        "        bdt_code = list(baudot_code)\n",
        "        var = bdt_code.copy()\n",
        "        while(True):\n",
        "          if (lgth > len(letters)-1) :\n",
        "            break\n",
        "          chr = letters[lgth]\n",
        "          # print(chr, lgth, letters)\n",
        "          if (chr in grp_0 or chr in grp_1):\n",
        "            # print(var[0], chr , grp_1 )\n",
        "            if (len(var) <=0) :\n",
        "              print(\"Need to regenerate\")\n",
        "              error = 1\n",
        "              break\n",
        "\n",
        "            elif ((chr in grp_0) and (var[0]!= '0')):\n",
        "              print(\"Need to regenerate\")\n",
        "              error = 1\n",
        "              break\n",
        "            elif ((chr in grp_1) and (var[0]!= '1')):\n",
        "              print(\"Need to regenerate\")\n",
        "              error = 1\n",
        "              break\n",
        "            else:\n",
        "              var.pop(0)\n",
        "          #loop over the word\n",
        "          lgth+=1\n",
        "\n",
        "        if(error == 0):\n",
        "            print(f'The remaining baudot code is \\n{var}')\n",
        "            return True, \"\".join(var)\n",
        "        return False,  \"\".join(bdt_code)\n",
        "\n",
        "    def remove_spaces(self, output) -> str:\n",
        "        lst = output[0].split('   ')[1:]\n",
        "        sentence = ''\n",
        "        for elm in lst:\n",
        "            for letter in elm:\n",
        "                if letter != ' ':\n",
        "                    sentence += letter\n",
        "                else:\n",
        "                    continue\n",
        "            sentence += ' '\n",
        "        return sentence\n",
        "    def gen_word(self,prompt):\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "        outputs = model.generate(inputs, pad_token_id=tokenizer.pad_token_id, max_new_tokens=10, do_sample=True, top_k=50, top_p=0.95)\n",
        "        return tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "    def stegger(self, secret_msg, initial_prompt):\n",
        "        self.set_keyword(secret_msg)\n",
        "        self.get_group()\n",
        "        self.encode()\n",
        "        baudot_code = self.baudot_code[secret_msg]\n",
        "        groups = self.groups[secret_msg]\n",
        "        grp_0 = groups[0]\n",
        "        grp_1 = groups[1]\n",
        "        ''' loop variables initialize'''\n",
        "        generated = self.gen_word(initial_prompt)\n",
        "        test_word = self.remove_spaces(generated)\n",
        "        remaining_bc = baudot_code\n",
        "        new_prompt = initial_prompt\n",
        "        self.cover = ''\n",
        "        j = 0\n",
        "        first_check = False\n",
        "        while True:\n",
        "            j += 1\n",
        "\n",
        "            if not j%20:\n",
        "                print(j, end='')\n",
        "            check, remaining_bc = self.check_word(test_word, remaining_bc, grp_1, grp_0)\n",
        "            if check:\n",
        "                new_prompt += test_word\n",
        "                if not first_check:\n",
        "                    self.cover = test_word\n",
        "                else:\n",
        "                    self.cover += test_word\n",
        "                first_check = True\n",
        "\n",
        "                generated = self.gen_word(self.cover)\n",
        "                test_word = self.remove_spaces(generated)\n",
        "            else:\n",
        "                generated = self.gen_word(self.cover)\n",
        "                test_word = self.remove_spaces(generated)\n",
        "\n",
        "            print(f'{self.cover}')\n",
        "            if len(remaining_bc) == 0:\n",
        "                print('Message has been hidden')\n",
        "                break\n",
        "        # return cover"
      ],
      "metadata": {
        "id": "WnUtv2bz7lao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hider = Stegonography()\n",
        "prompt =\"میں\"\n",
        "secret = 'حبیب'\n",
        "hider.stegger(secret, prompt)"
      ],
      "metadata": {
        "id": "qzomjl2EErka",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3d88425-167c-4840-c509-7efd43d01e94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "حبیب\n",
            "The code is \n",
            " and The letter = ح\n",
            "The code is \n",
            "001000 and The letter = ب\n",
            "The code is \n",
            "001000000001 and The letter = ی\n",
            "The code is \n",
            "001000000001100100 and The letter = ب\n",
            "['ا', 'س', 'ے', ' '] اسے \n",
            "The remaining baudot code is \n",
            "['0', '1', '0', '0', '0', '0', '0', '0', '0', '0', '1', '1', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '1']\n",
            "اسے \n",
            "['ک', 'ر', 'ت', 'ا', ' '] کرتا \n",
            "Need to regenerate\n",
            "اسے \n",
            "['ک', 'و', ' ', 'گ', ' '] کو گ \n",
            "Need to regenerate\n",
            "اسے \n",
            "['م', 'ی', 'ٹ', 'ا', ' '] میٹا \n",
            "Need to regenerate\n",
            "اسے \n",
            "['ک', 'ی', 'و', 'ن', ' '] کیون \n",
            "Need to regenerate\n",
            "اسے \n",
            "['چ', 'ھ', 'و', 'ٹ', ' '] چھوٹ \n",
            "The remaining baudot code is \n",
            "['0', '0', '0', '0', '0', '0', '0', '1', '1', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '1']\n",
            "اسے چھوٹ \n",
            "['ک', 'ے', ' ', ' '] کے  \n",
            "Need to regenerate\n",
            "اسے چھوٹ \n",
            "['م', ' '] م \n",
            "Need to regenerate\n",
            "اسے چھوٹ \n",
            "['ک', 'ر', '�', ' '] کر� \n",
            "Need to regenerate\n",
            "اسے چھوٹ \n",
            "['ش', 'ی', '�', ' '] شی� \n",
            "Need to regenerate\n",
            "اسے چھوٹ \n",
            "['ہ', ' '] ہ \n",
            "The remaining baudot code is \n",
            "['0', '0', '0', '0', '0', '0', '0', '1', '1', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '1']\n",
            "اسے چھوٹ ہ \n",
            "['ن', 'ہ', 'ی', ' '] نہی \n",
            "Need to regenerate\n",
            "اسے چھوٹ ہ \n",
            "['ا', '�', ' '] ا� \n",
            "The remaining baudot code is \n",
            "['0', '0', '0', '0', '0', '0', '1', '1', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '1']\n",
            "اسے چھوٹ ہ ا� \n",
            "['ت', 'ھ', 'ی', ' '] تھی \n",
            "Need to regenerate\n",
            "اسے چھوٹ ہ ا� \n",
            "['ک', 'ی', ' ', ' '] کی  \n",
            "Need to regenerate\n",
            "اسے چھوٹ ہ ا� \n",
            "['م', 'ی', 'ں', ' '] میں \n",
            "Need to regenerate\n",
            "اسے چھوٹ ہ ا� \n",
            "['چ', 'ھ', 'و', '�', ' '] چھو� \n",
            "Need to regenerate\n",
            "اسے چھوٹ ہ ا� \n",
            "['ہ', 'ے', '۔', ' '] ہے۔ \n",
            "The remaining baudot code is \n",
            "['0', '0', '0', '0', '0', '0', '1', '1', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '1']\n",
            "اسے چھوٹ ہ ا� ہے۔ \n",
            "['س', 'ب', ' ', 'ک', ' '] سب ک \n",
            "Need to regenerate\n",
            "اسے چھوٹ ہ ا� ہے۔ \n",
            "20['ہ', 'ی', 'ں', ' '] ہیں \n",
            "Need to regenerate\n",
            "اسے چھوٹ ہ ا� ہے۔ \n",
            "['ج', 'ن', 'ہ', 'و', ' '] جنہو \n",
            "The remaining baudot code is \n",
            "['0', '0', '0', '0', '0', '1', '1', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '1']\n",
            "اسے چھوٹ ہ ا� ہے۔ جنہو \n",
            "[] \n",
            "The remaining baudot code is \n",
            "['0', '0', '0', '0', '0', '1', '1', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '1']\n",
            "اسے چھوٹ ہ ا� ہے۔ جنہو \n",
            "[' ']  \n",
            "The remaining baudot code is \n",
            "['0', '0', '0', '0', '0', '1', '1', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '1']\n",
            "اسے چھوٹ ہ ا� ہے۔ جنہو  \n",
            "['ت', 'ھ', 'ی', '�', ' '] تھی� \n",
            "Need to regenerate\n",
            "اسے چھوٹ ہ ا� ہے۔ جنہو  \n",
            "['ٹ', 'و', 'ر', 'ی', '�', ' '] ٹوری� \n",
            "Need to regenerate\n",
            "اسے چھوٹ ہ ا� ہے۔ جنہو  \n",
            "['ف', 'ل', 'م', ' ', 'ک', ' '] فلم ک \n",
            "Need to regenerate\n",
            "اسے چھوٹ ہ ا� ہے۔ جنہو  \n",
            "['ف', 'ل', 'م', ' ', 'م', ' '] فلم م \n",
            "Need to regenerate\n",
            "اسے چھوٹ ہ ا� ہے۔ جنہو  \n",
            "['م', 'ی', 'ن', ' ', 'ا', ' '] مین ا \n",
            "Need to regenerate\n",
            "اسے چھوٹ ہ ا� ہے۔ جنہو  \n",
            "['ک', 'ے', ' ', 'ب', ' '] کے ب \n",
            "Need to regenerate\n",
            "اسے چھوٹ ہ ا� ہے۔ جنہو  \n",
            "['آ', 'ف', ' ', 'د', 'ر', ' '] آف در \n",
            "The remaining baudot code is \n",
            "['0', '0', '0', '0', '1', '1', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '1']\n",
            "اسے چھوٹ ہ ا� ہے۔ جنہو  آف در \n",
            "['ہ', ' '] ہ \n",
            "The remaining baudot code is \n",
            "['0', '0', '0', '0', '1', '1', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '1']\n",
            "اسے چھوٹ ہ ا� ہے۔ جنہو  آف در ہ \n",
            "['ف', 'ل', 'م', ' ', '�', ' '] فلم � \n",
            "Need to regenerate\n",
            "اسے چھوٹ ہ ا� ہے۔ جنہو  آف در ہ \n",
            "['ج', 'ی', 'م', ' ', 'و', ' '] جیم و \n",
            "Need to regenerate\n",
            "اسے چھوٹ ہ ا� ہے۔ جنہو  آف در ہ \n",
            "[' ']  \n",
            "The remaining baudot code is \n",
            "['0', '0', '0', '0', '1', '1', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '1']\n",
            "اسے چھوٹ ہ ا� ہے۔ جنہو  آف در ہ  \n",
            "['س', 'ے', ' ', 'ہ', ' '] سے ہ \n",
            "The remaining baudot code is \n",
            "['0', '0', '0', '0', '1', '1', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '1']\n",
            "اسے چھوٹ ہ ا� ہے۔ جنہو  آف در ہ  سے ہ \n",
            "['ا', 'ی', 'ک', ' '] ایک \n",
            "Need to regenerate\n",
            "اسے چھوٹ ہ ا� ہے۔ جنہو  آف در ہ  سے ہ \n",
            "['و', 'ا', 'ق', 'ع', 'ت', ' '] واقعت \n",
            "The remaining baudot code is \n",
            "['0', '0', '1', '1', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '1']\n",
            "اسے چھوٹ ہ ا� ہے۔ جنہو  آف در ہ  سے ہ واقعت \n",
            "['خ', 'ر', '�', ' '] خر� \n",
            "The remaining baudot code is \n",
            "['0', '0', '1', '1', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '1']\n",
            "اسے چھوٹ ہ ا� ہے۔ جنہو  آف در ہ  سے ہ واقعت خر� \n",
            "['�', ' '] � \n",
            "The remaining baudot code is \n",
            "['0', '0', '1', '1', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '1']\n",
            "اسے چھوٹ ہ ا� ہے۔ جنہو  آف در ہ  سے ہ واقعت خر� � \n",
            "40['م', 'ی', 'ر', 'ی', ' '] میری \n",
            "Need to regenerate\n",
            "اسے چھوٹ ہ ا� ہے۔ جنہو  آف در ہ  سے ہ واقعت خر� � \n",
            "['م', 'ی', 'ں', ' '] میں \n",
            "Need to regenerate\n",
            "اسے چھوٹ ہ ا� ہے۔ جنہو  آف در ہ  سے ہ واقعت خر� � \n",
            "['پ', 'ر', ' ', 'ر', 'و', ' '] پر رو \n",
            "Need to regenerate\n",
            "اسے چھوٹ ہ ا� ہے۔ جنہو  آف در ہ  سے ہ واقعت خر� � \n",
            "['ا', 'د', 'ا', 'ک', ' '] اداک \n",
            "The remaining baudot code is \n",
            "['1', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '1']\n",
            "اسے چھوٹ ہ ا� ہے۔ جنہو  آف در ہ  سے ہ واقعت خر� � اداک \n",
            "['آ', 'پ', ' '] آپ \n",
            "Need to regenerate\n",
            "اسے چھوٹ ہ ا� ہے۔ جنہو  آف در ہ  سے ہ واقعت خر� � اداک \n",
            "['ب', 'ھ', ' '] بھ \n",
            "The remaining baudot code is \n",
            "['0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '1']\n",
            "اسے چھوٹ ہ ا� ہے۔ جنہو  آف در ہ  سے ہ واقعت خر� � اداک بھ \n",
            "['ہ', 'ر', ' ', 'ع', ' '] ہر ع \n",
            "The remaining baudot code is \n",
            "['0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '1']\n",
            "اسے چھوٹ ہ ا� ہے۔ جنہو  آف در ہ  سے ہ واقعت خر� � اداک بھ ہر ع \n",
            "['م', 'ح', 'ب', '�', ' '] محب� \n",
            "Need to regenerate\n",
            "اسے چھوٹ ہ ا� ہے۔ جنہو  آف در ہ  سے ہ واقعت خر� � اداک بھ ہر ع \n",
            "['م', 'ی', '�', ' '] می� \n",
            "Need to regenerate\n",
            "اسے چھوٹ ہ ا� ہے۔ جنہو  آف در ہ  سے ہ واقعت خر� � اداک بھ ہر ع \n",
            "['ہ', 'م', '�', ' '] ہم� \n",
            "Need to regenerate\n",
            "اسے چھوٹ ہ ا� ہے۔ جنہو  آف در ہ  سے ہ واقعت خر� � اداک بھ ہر ع \n",
            "['ک', 'ا', ' '] کا \n",
            "Need to regenerate\n",
            "اسے چھوٹ ہ ا� ہے۔ جنہو  آف در ہ  سے ہ واقعت خر� � اداک بھ ہر ع \n",
            "['ک', 'و', ' '] کو \n",
            "Need to regenerate\n",
            "اسے چھوٹ ہ ا� ہے۔ جنہو  آف در ہ  سے ہ واقعت خر� � اداک بھ ہر ع \n",
            "['ک', 'ا', ' '] کا \n",
            "Need to regenerate\n",
            "اسے چھوٹ ہ ا� ہے۔ جنہو  آف در ہ  سے ہ واقعت خر� � اداک بھ ہر ع \n",
            "[] \n",
            "The remaining baudot code is \n",
            "['0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '1']\n",
            "اسے چھوٹ ہ ا� ہے۔ جنہو  آف در ہ  سے ہ واقعت خر� � اداک بھ ہر ع \n",
            "['ا', 'ی', ' '] ای \n",
            "Need to regenerate\n",
            "اسے چھوٹ ہ ا� ہے۔ جنہو  آف در ہ  سے ہ واقعت خر� � اداک بھ ہر ع \n",
            "['ا', 'و', 'ر', ' '] اور \n",
            "The remaining baudot code is \n",
            "['1', '0', '0', '0', '0', '0', '0', '0', '1']\n",
            "اسے چھوٹ ہ ا� ہے۔ جنہو  آف در ہ  سے ہ واقعت خر� � اداک بھ ہر ع اور \n",
            "['ب', '�', ' '] ب� \n",
            "The remaining baudot code is \n",
            "['1', '0', '0', '0', '0', '0', '0', '0', '1']\n",
            "اسے چھوٹ ہ ا� ہے۔ جنہو  آف در ہ  سے ہ واقعت خر� � اداک بھ ہر ع اور ب� \n",
            "['ک', 'ے', ' ', '�', ' '] کے � \n",
            "The remaining baudot code is \n",
            "['0', '0', '0', '0', '0', '0', '0', '1']\n",
            "اسے چھوٹ ہ ا� ہے۔ جنہو  آف در ہ  سے ہ واقعت خر� � اداک بھ ہر ع اور ب� کے � \n",
            "['ہ', 'د', 'ا', '�', ' '] ہدا� \n",
            "The remaining baudot code is \n",
            "['0', '0', '0', '0', '0', '0', '1']\n",
            "اسے چھوٹ ہ ا� ہے۔ جنہو  آف در ہ  سے ہ واقعت خر� � اداک بھ ہر ع اور ب� کے � ہدا� \n",
            "['ی', 'ہ', 'ا', ' '] یہا \n",
            "Need to regenerate\n",
            "اسے چھوٹ ہ ا� ہے۔ جنہو  آف در ہ  سے ہ واقعت خر� � اداک بھ ہر ع اور ب� کے � ہدا� \n",
            "60['ک', 'م', ' '] کم \n",
            "Need to regenerate\n",
            "اسے چھوٹ ہ ا� ہے۔ جنہو  آف در ہ  سے ہ واقعت خر� � اداک بھ ہر ع اور ب� کے � ہدا� \n",
            "['ڈ', ' '] ڈ \n",
            "The remaining baudot code is \n",
            "['0', '0', '0', '0', '0', '0', '1']\n",
            "اسے چھوٹ ہ ا� ہے۔ جنہو  آف در ہ  سے ہ واقعت خر� � اداک بھ ہر ع اور ب� کے � ہدا� ڈ \n",
            "['�', ' '] � \n",
            "The remaining baudot code is \n",
            "['0', '0', '0', '0', '0', '0', '1']\n",
            "اسے چھوٹ ہ ا� ہے۔ جنہو  آف در ہ  سے ہ واقعت خر� � اداک بھ ہر ع اور ب� کے � ہدا� ڈ � \n",
            "[] \n",
            "The remaining baudot code is \n",
            "['0', '0', '0', '0', '0', '0', '1']\n",
            "اسے چھوٹ ہ ا� ہے۔ جنہو  آف در ہ  سے ہ واقعت خر� � اداک بھ ہر ع اور ب� کے � ہدا� ڈ � \n",
            "['ک', 'ے', ' ', '�', ' '] کے � \n",
            "Need to regenerate\n",
            "اسے چھوٹ ہ ا� ہے۔ جنہو  آف در ہ  سے ہ واقعت خر� � اداک بھ ہر ع اور ب� کے � ہدا� ڈ � \n",
            "['ک', 'ی', ' ', ' '] کی  \n",
            "Need to regenerate\n",
            "اسے چھوٹ ہ ا� ہے۔ جنہو  آف در ہ  سے ہ واقعت خر� � اداک بھ ہر ع اور ب� کے � ہدا� ڈ � \n",
            "['ز', 'ن', 'د', 'گ', ' '] زندگ \n",
            "The remaining baudot code is \n",
            "['0', '0', '0', '0', '0', '0', '1']\n",
            "اسے چھوٹ ہ ا� ہے۔ جنہو  آف در ہ  سے ہ واقعت خر� � اداک بھ ہر ع اور ب� کے � ہدا� ڈ � زندگ \n",
            "[] \n",
            "The remaining baudot code is \n",
            "['0', '0', '0', '0', '0', '0', '1']\n",
            "اسے چھوٹ ہ ا� ہے۔ جنہو  آف در ہ  سے ہ واقعت خر� � اداک بھ ہر ع اور ب� کے � ہدا� ڈ � زندگ \n",
            "['ا', 'س', 'پ', ' '] اسپ \n",
            "Need to regenerate\n",
            "اسے چھوٹ ہ ا� ہے۔ جنہو  آف در ہ  سے ہ واقعت خر� � اداک بھ ہر ع اور ب� کے � ہدا� ڈ � زندگ \n",
            "['پ', '�', ' '] پ� \n",
            "Need to regenerate\n",
            "اسے چھوٹ ہ ا� ہے۔ جنہو  آف در ہ  سے ہ واقعت خر� � اداک بھ ہر ع اور ب� کے � ہدا� ڈ � زندگ \n",
            "['ٹ', 'ی', 'ن', '�', ' '] ٹین� \n",
            "Need to regenerate\n",
            "اسے چھوٹ ہ ا� ہے۔ جنہو  آف در ہ  سے ہ واقعت خر� � اداک بھ ہر ع اور ب� کے � ہدا� ڈ � زندگ \n",
            "['م', 'ل', 'ے', ' ', '�', ' '] ملے � \n",
            "Need to regenerate\n",
            "اسے چھوٹ ہ ا� ہے۔ جنہو  آف در ہ  سے ہ واقعت خر� � اداک بھ ہر ع اور ب� کے � ہدا� ڈ � زندگ \n",
            "[' ']  \n",
            "The remaining baudot code is \n",
            "['0', '0', '0', '0', '0', '0', '1']\n",
            "اسے چھوٹ ہ ا� ہے۔ جنہو  آف در ہ  سے ہ واقعت خر� � اداک بھ ہر ع اور ب� کے � ہدا� ڈ � زندگ  \n",
            "['ا', 'س', ' ', 'ک', ' '] اس ک \n",
            "Need to regenerate\n",
            "اسے چھوٹ ہ ا� ہے۔ جنہو  آف در ہ  سے ہ واقعت خر� � اداک بھ ہر ع اور ب� کے � ہدا� ڈ � زندگ  \n",
            "['آ', 'ن', 'ے', ' ', ' '] آنے  \n",
            "The remaining baudot code is \n",
            "['0', '0', '0', '0', '0', '1']\n",
            "اسے چھوٹ ہ ا� ہے۔ جنہو  آف در ہ  سے ہ واقعت خر� � اداک بھ ہر ع اور ب� کے � ہدا� ڈ � زندگ  آنے  \n",
            "['ا', 'د', 'ا', 'ک', ' '] اداک \n",
            "Need to regenerate\n",
            "اسے چھوٹ ہ ا� ہے۔ جنہو  آف در ہ  سے ہ واقعت خر� � اداک بھ ہر ع اور ب� کے � ہدا� ڈ � زندگ  آنے  \n",
            "['ی', 'ہ', ' ', 'ا', ' '] یہ ا \n",
            "Need to regenerate\n",
            "اسے چھوٹ ہ ا� ہے۔ جنہو  آف در ہ  سے ہ واقعت خر� � اداک بھ ہر ع اور ب� کے � ہدا� ڈ � زندگ  آنے  \n",
            "['ا', 'و', 'ر', ' ', 'ا', '�', ' '] اور ا� \n",
            "The remaining baudot code is \n",
            "['0', '0', '1']\n",
            "اسے چھوٹ ہ ا� ہے۔ جنہو  آف در ہ  سے ہ واقعت خر� � اداک بھ ہر ع اور ب� کے � ہدا� ڈ � زندگ  آنے  اور ا� \n",
            "['ا', 'ف', 'ر', ' '] افر \n",
            "The remaining baudot code is \n",
            "['0', '1']\n",
            "اسے چھوٹ ہ ا� ہے۔ جنہو  آف در ہ  سے ہ واقعت خر� � اداک بھ ہر ع اور ب� کے � ہدا� ڈ � زندگ  آنے  اور ا� افر \n",
            "['�', ' '] � \n",
            "The remaining baudot code is \n",
            "['0', '1']\n",
            "اسے چھوٹ ہ ا� ہے۔ جنہو  آف در ہ  سے ہ واقعت خر� � اداک بھ ہر ع اور ب� کے � ہدا� ڈ � زندگ  آنے  اور ا� افر � \n",
            "80[] \n",
            "The remaining baudot code is \n",
            "['0', '1']\n",
            "اسے چھوٹ ہ ا� ہے۔ جنہو  آف در ہ  سے ہ واقعت خر� � اداک بھ ہر ع اور ب� کے � ہدا� ڈ � زندگ  آنے  اور ا� افر � \n",
            "['ج', 'و', ' '] جو \n",
            "The remaining baudot code is \n",
            "['1']\n",
            "اسے چھوٹ ہ ا� ہے۔ جنہو  آف در ہ  سے ہ واقعت خر� � اداک بھ ہر ع اور ب� کے � ہدا� ڈ � زندگ  آنے  اور ا� افر � جو \n",
            "['ا', 'س', ' ', '�', ' '] اس � \n",
            "Need to regenerate\n",
            "اسے چھوٹ ہ ا� ہے۔ جنہو  آف در ہ  سے ہ واقعت خر� � اداک بھ ہر ع اور ب� کے � ہدا� ڈ � زندگ  آنے  اور ا� افر � جو \n",
            "['م', 'ع', 'م', 'و', 'ل', '�', ' '] معمول� \n",
            "Need to regenerate\n",
            "اسے چھوٹ ہ ا� ہے۔ جنہو  آف در ہ  سے ہ واقعت خر� � اداک بھ ہر ع اور ب� کے � ہدا� ڈ � زندگ  آنے  اور ا� افر � جو \n",
            "['س', ' '] س \n",
            "The remaining baudot code is \n",
            "['1']\n",
            "اسے چھوٹ ہ ا� ہے۔ جنہو  آف در ہ  سے ہ واقعت خر� � اداک بھ ہر ع اور ب� کے � ہدا� ڈ � زندگ  آنے  اور ا� افر � جو س \n",
            "[] \n",
            "The remaining baudot code is \n",
            "['1']\n",
            "اسے چھوٹ ہ ا� ہے۔ جنہو  آف در ہ  سے ہ واقعت خر� � اداک بھ ہر ع اور ب� کے � ہدا� ڈ � زندگ  آنے  اور ا� افر � جو س \n",
            "['م', 'ی', 'ں', ' '] میں \n",
            "Need to regenerate\n",
            "اسے چھوٹ ہ ا� ہے۔ جنہو  آف در ہ  سے ہ واقعت خر� � اداک بھ ہر ع اور ب� کے � ہدا� ڈ � زندگ  آنے  اور ا� افر � جو س \n",
            "['م', 'ی', 'ں', ' '] میں \n",
            "Need to regenerate\n",
            "اسے چھوٹ ہ ا� ہے۔ جنہو  آف در ہ  سے ہ واقعت خر� � اداک بھ ہر ع اور ب� کے � ہدا� ڈ � زندگ  آنے  اور ا� افر � جو س \n",
            "['ک', 'ے', ' ', 'م', ' '] کے م \n",
            "Need to regenerate\n",
            "اسے چھوٹ ہ ا� ہے۔ جنہو  آف در ہ  سے ہ واقعت خر� � اداک بھ ہر ع اور ب� کے � ہدا� ڈ � زندگ  آنے  اور ا� افر � جو س \n",
            "['ب', 'ی', 'ک', '�', ' '] بیک� \n",
            "Need to regenerate\n",
            "اسے چھوٹ ہ ا� ہے۔ جنہو  آف در ہ  سے ہ واقعت خر� � اداک بھ ہر ع اور ب� کے � ہدا� ڈ � زندگ  آنے  اور ا� افر � جو س \n",
            "['م', 'ی', 'ں', ' '] میں \n",
            "Need to regenerate\n",
            "اسے چھوٹ ہ ا� ہے۔ جنہو  آف در ہ  سے ہ واقعت خر� � اداک بھ ہر ع اور ب� کے � ہدا� ڈ � زندگ  آنے  اور ا� افر � جو س \n",
            "['ک', 'ے', ' '] کے \n",
            "The remaining baudot code is \n",
            "[]\n",
            "اسے چھوٹ ہ ا� ہے۔ جنہو  آف در ہ  سے ہ واقعت خر� � اداک بھ ہر ع اور ب� کے � ہدا� ڈ � زندگ  آنے  اور ا� افر � جو س کے \n",
            "Message has been hidden\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xx = ''\n",
        "for elm in secret:\n",
        "  x = hider.urdu_to_baudot_mapping[elm]\n",
        "  print(f'{elm} = {hider.urdu_to_baudot_mapping[elm]}')\n",
        "  xx += x\n",
        "print(f'The complete binary representation of the word {secret} is:\\n{xx}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3GrGQPVfmD0I",
        "outputId": "baac827d-1371-4db8-a09d-27184f129ac2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ح = 001000\n",
            "ب = 000001\n",
            "ی = 100100\n",
            "ب = 000001\n",
            "The complete binary representation of the word حبیب is:\n",
            "001000000001100100000001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "string = hider.cover\n",
        "xxx = tokenizer(string, return_tensors=\"pt\").input_ids\n",
        "print(string)\n",
        "print(hider.groups[secret][1])\n",
        "print(hider.baudot_code)\n",
        "code = ''\n",
        "group_0 = hider.groups[secret][0]\n",
        "group_1 = hider.groups[secret][1]\n",
        "for letter in string:\n",
        "    if letter in group_0:\n",
        "        code += '0'\n",
        "    elif letter in group_1:\n",
        "        code += '1'\n",
        "    else:\n",
        "        continue\n",
        "\n",
        "print(code)\n",
        "# i = 0\n",
        "# for elm in string:\n",
        "#     print(elm, i)\n",
        "#     i += 1"
      ],
      "metadata": {
        "id": "I6Q6OX_lIqOK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efe3be80-3170-4c3f-d48c-8f6634035f9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "اسے چھوٹ ہ ا� ہے۔ جنہو  آف در ہ  سے ہ واقعت خر� � اداک بھ ہر ع اور ب� کے � ہدا� ڈ � زندگ  آنے  اور ا� افر � جو س کے \n",
            "['ھ', 'ک', 'ی', 'م', 'پ']\n",
            "{'حبیب': '001000000001100100000001'}\n",
            "001000000001100100000001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(group_0)\n",
        "print(group_1)"
      ],
      "metadata": {
        "id": "mZNF7dp-SE7z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ae8f4ee-b71b-4524-a7b9-e64e0f437dd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ں', 'آ', 'چ', 'و', 'ا']\n",
            "['ھ', 'ک', 'ی', 'م', 'پ']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hider.decode(code)\n",
        "hider.encode()"
      ],
      "metadata": {
        "id": "Jz_t8vUQV-C7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7d2dbf0-6f16-4e80-d61b-cb5e955851e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the code is 001000 and the letter is ح\n",
            "the code is 000001 and the letter is ب\n",
            "the code is 100100 and the letter is ی\n",
            "the code is 000001 and the letter is ب\n",
            "['ح', 'ب', 'ی', 'ب']\n",
            "حبیب\n",
            "The code is \n",
            " and The letter = ح\n",
            "The code is \n",
            "001000 and The letter = ب\n",
            "The code is \n",
            "001000000001 and The letter = ی\n",
            "The code is \n",
            "001000000001100100 and The letter = ب\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('The secret message:', secret)\n",
        "print()\n",
        "print('The key used is: ')\n",
        "print('group_0:', group_0)\n",
        "print('group_1:', group_1)\n",
        "print('The algorithm converts the secret message into the following: ')\n",
        "hider.encode()\n",
        "print()\n",
        "print('The language model produces the following cover text: ')\n",
        "print(hider.cover)\n",
        "print()\n",
        "print('The secret message extracted by the algorithm: ')\n",
        "hider.decode(code)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "200w9UejZfOc",
        "outputId": "6b8dd7aa-c68f-4dac-c15a-e79cabd78bc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The secret message: حبیب\n",
            "\n",
            "The key used is: \n",
            "group_0: ['ں', 'آ', 'چ', 'و', 'ا']\n",
            "group_1: ['ھ', 'ک', 'ی', 'م', 'پ']\n",
            "The algorithm converts the secret message into the following: \n",
            "حبیب\n",
            "The code is \n",
            " and The letter = ح\n",
            "The code is \n",
            "001000 and The letter = ب\n",
            "The code is \n",
            "001000000001 and The letter = ی\n",
            "The code is \n",
            "001000000001100100 and The letter = ب\n",
            "\n",
            "The language model produces the following cover text: \n",
            "اسے چھوٹ ہ ا� ہے۔ جنہو  آف در ہ  سے ہ واقعت خر� � اداک بھ ہر ع اور ب� کے � ہدا� ڈ � زندگ  آنے  اور ا� افر � جو س کے \n",
            "\n",
            "The secret message extracted by the algorithm: \n",
            "the code is 001000 and the letter is ح\n",
            "the code is 000001 and the letter is ب\n",
            "the code is 100100 and the letter is ی\n",
            "the code is 000001 and the letter is ب\n",
            "['ح', 'ب', 'ی', 'ب']\n"
          ]
        }
      ]
    }
  ]
}